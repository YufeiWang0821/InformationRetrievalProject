{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 13,
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import dataset2dataloader\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
<<<<<<< HEAD
    "from torchcrf import CRF\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.nn import init\n",
    "from torchtext.legacy import data"
=======
    "from transformers import BertModel, BertTokenizer\n",
    "from torchcrf import CRF\n",
    "import numpy as np\n",
    "import os"
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_tags, word_vectors=None, device=\"cpu\"):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, _weight=word_vectors).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, bidirectional=True, batch_first=True).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_size*2, num_tags)\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True).to(device)\n",
    "\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_CRF(nn.Module):\n",
    "    def __init__(self, num_tags, device=\"cpu\"):\n",
    "        super(BERT_CRF, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_tags = num_tags\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.hidden2tag = nn.Linear(self.bert.config.hidden_size, num_tags)\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True).to(device)\n",
    "\n",
    "    def get_emissions(self, x):\n",
    "        embeddings = self.bert(x)[0]\n",
    "        emissions = self.hidden2tag(embeddings)\n",
    "        return emissions\n",
    "    \n",
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
    "    def forward(self, x, y, mask):\n",
    "        emissions = self.get_emissions(x)\n",
    "        loss = -self.crf(emissions=emissions, tags=y, mask=mask)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x, mask=None):\n",
    "        emissions = self.get_emissions(x)\n",
    "        preds = self.crf.decode(emissions, mask)\n",
<<<<<<< HEAD
    "        return preds\n",
    "\n",
    "    def get_emissions(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        embedded = self.embed(x)\n",
    "        h0, c0 = torch.zeros(2, batch_size, self.hidden_size).to(self.device), torch.zeros(2, batch_size, self.hidden_size).to(self.device)\n",
    "        lstm_out, (_, _) = self.lstm(embedded, (h0, c0))\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return emissions"
=======
    "        return preds"
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def prepare_data(dataset_path, debug=False):\n",
    "    train_file_path = os.path.join(dataset_path, \"train.txt\")\n",
    "    dev_file_path = os.path.join(dataset_path, \"dev.txt\")\n",
    "\n",
    "    def process_file(file_path, target_file_path):\n",
    "        sents, tags = [], []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            sent, tag = [], []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if len(line) == 0:\n",
    "                    sents.append(\" \".join(sent))\n",
    "                    tags.append(\" \".join(tag))\n",
    "                    sent, tag = [], []\n",
    "                else:\n",
    "                    splited = line.split(\" \")\n",
    "                    sent.append(splited[0])\n",
    "                    tag.append(splited[-1])\n",
    "            if len(sent) != 0:\n",
    "                sents.append(\" \".join(sent))\n",
    "                tags.append(\" \".join(tag))\n",
    "        df = pd.DataFrame()\n",
    "        df[\"sent\"] = sents if not debug else sents[:100]\n",
    "        df[\"tag\"] = tags if not debug else tags[:100]\n",
    "        df.to_csv(target_file_path, index=False)\n",
    "\n",
    "    train_csv = os.path.join(dataset_path, \"train.csv\") if not debug else os.path.join(dataset_path, \"train_small.csv\")\n",
    "    dev_csv = os.path.join(dataset_path, \"dev.csv\") if not debug else os.path.join(dataset_path, \"train_dev.csv\")\n",
    "\n",
    "    if not os.path.exists(train_csv):\n",
    "        process_file(train_file_path, train_csv)\n",
    "        process_file(dev_file_path, dev_csv)\n",
    "\n",
    "    return train_csv, dev_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset2dataloader(dataset_path=\"/data/wyf/project3-Named Entity Recognition/data/\", batch_size=3, debug=False):\n",
    "    train_csv, dev_csv = prepare_data(dataset_path, debug=debug)\n",
    "\n",
    "    def tokenizer(text):\n",
    "        return text.split(\" \")\n",
    "\n",
    "    # 这里只是定义了数据格式\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=False)\n",
    "    TAG = data.Field(sequential=True, tokenize=tokenizer, lower=False)\n",
    "    train, val = data.TabularDataset.splits(\n",
    "        path='', train=train_csv, validation=dev_csv, format='csv', skip_header=True,\n",
    "        fields=[('sent', TEXT), ('tag', TAG)])\n",
    "\n",
    "    TEXT.build_vocab(train, vectors='glove.6B.50d')  # , max_size=30000)\n",
    "    TAG.build_vocab(val)\n",
    "\n",
    "    # 当 corpus 中有的 token 在 vectors 中不存在时 的初始化方式.\n",
    "    TEXT.vocab.vectors.unk_init = init.xavier_uniform\n",
    "\n",
    "    DEVICE = \"cpu\"\n",
    "    train_iter = data.BucketIterator(train, batch_size=batch_size, sort_key=lambda x: len(x.sent), device=DEVICE)\n",
    "    val_iter = data.BucketIterator(val, batch_size=batch_size, sort_key=lambda x: len(x.sent), device=DEVICE)\n",
    "\n",
    "    # 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序\n",
    "    # test_iter = data.Iterator(dataset=test, batch_size=128, train=False, sort=False, device=DEVICE)\n",
    "\n",
    "    return train_iter, val_iter, TEXT.vocab, TAG.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
    "train_iter, val_iter, sent_vocab, tag_vocab = dataset2dataloader(batch_size=128)\n",
    "word_vectors = sent_vocab.vectors\n",
    "\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "这里由于BiLSTM的数据处理问题，出现了如下报错  \n",
    "`RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor`  \n",
    "这似乎是由于在进行文本嵌入的时候，调用的`nn.Embedding`函数的问题  \n",
    "由于**时间**关系我暂时只能先使`device = torch.device('cpu')`  \n",
    "按照老师所说，后面的NLP专选课实验也会使用BiLSTM+CRF实现NER，届时可能会解决这一问题"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
    "训练"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(vocab_size=len(sent_vocab.stoi), embedding_dim=50, hidden_size=128, num_tags=len(tag_vocab.stoi), word_vectors=word_vectors, device=device)"
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_CRF(vocab_size=len(sent_vocab.stoi), embedding_dim=50, hidden_size=128, num_tags=len(tag_vocab.stoi), word_vectors=word_vectors, device=device)"
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 16,
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "learning_rate = 0.01\n",
<<<<<<< HEAD
    "model_path = \"model.pkl\""
=======
    "model_path = \"model_BertC.pkl\""
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 17,
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, iter:0, loss:5228.29833984375 epoch:0, iter:100, loss:342.710205078125 epoch 10 train acc:0.97, val acc:0.94\n",
      "epoch:1, iter:0, loss:211.81451416015625 epoch:1, iter:100, loss:70.90070343017578 epoch 10 train acc:0.99, val acc:0.96\n",
      "epoch:2, iter:0, loss:72.41127014160156 epoch:2, iter:100, loss:44.5744743347168 epoch 10 train acc:1.00, val acc:0.96\n",
      "epoch:3, iter:0, loss:19.108728408813477 epoch:3, iter:100, loss:35.543296813964844 epoch 10 train acc:1.00, val acc:0.97\n",
      "epoch:4, iter:0, loss:16.934940338134766 epoch:4, iter:100, loss:11.126664161682129 epoch 10 train acc:1.00, val acc:0.96\n",
      "epoch:5, iter:0, loss:4.5881476402282715 epoch:5, iter:100, loss:5.764562606811523 epoch 10 train acc:1.00, val acc:0.96\n",
      "epoch:6, iter:0, loss:1.6615972518920898 epoch:6, iter:100, loss:1.5588173866271973 epoch 10 train acc:1.00, val acc:0.95\n",
      "epoch:7, iter:0, loss:1.6595497131347656 epoch:7, iter:100, loss:0.116668701171875 epoch 10 train acc:1.00, val acc:0.96\n",
      "epoch:8, iter:0, loss:0.49768829345703125 epoch:8, iter:100, loss:0.48386573791503906 epoch 10 train acc:1.00, val acc:0.96\n",
      "epoch:9, iter:0, loss:0.3464536666870117 epoch:9, iter:100, loss:0.2817878723144531 epoch 10 train acc:1.00, val acc:0.96\n"
     ]
    }
   ],
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   "source": [
    "if os.path.exists(model_path):\n",
    "    model = torch.load(model_path)\n",
    "else:\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            x, y = batch.sent.t(), batch.tag.t()\n",
    "            mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(x, y, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                print(f\"epoch:{ep}, iter:{i}, loss:{loss.item()}\", end=\" \")\n",
    "\n",
    "        model.eval()\n",
    "        train_accs = []\n",
    "        preds, golds = [], []\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            x, y = batch.sent.t(), batch.tag.t()\n",
    "            mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "            with torch.no_grad():\n",
    "                preds = model.predict(x, mask)\n",
    "            right, total = 0, 0\n",
    "            for pred, gold in zip(preds, y):\n",
    "                right += np.sum(np.array(pred) == gold[:len(pred)].numpy())\n",
    "                total += len(pred)\n",
    "            train_accs.append(right*1.0/total)\n",
    "        train_acc = np.array(train_accs).mean()\n",
    "\n",
    "        val_accs = []\n",
    "        for i, batch in enumerate(val_iter):\n",
    "            x, y = batch.sent.t(), batch.tag.t()\n",
    "            mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "            with torch.no_grad():\n",
    "                preds = model.predict(x, mask)\n",
    "            right, total = 0, 0\n",
    "            for pred, gold in zip(preds, y):\n",
    "                right += np.sum(np.array(pred) == gold[:len(pred)].numpy())\n",
    "                total += len(pred)\n",
    "            val_accs.append(right * 1.0 / total)\n",
    "        val_acc = np.array(val_accs).mean()\n",
    "        print(\"epoch %d train acc:%.2f, val acc:%.2f\" % (epoch, train_acc, val_acc))\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 43,
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sents = [\"My name is Yufei Wang , I am from Jinzhou , Hubei , China .\"]\n",
    "#test_sents = [\"HUST is the abbreviation of Huazhong University of Science and Technology . It is located in Hongshan , Wuhan , China .\"]\n",
    "test_sents = [\"Sufjan Stevens released his thirteenth album Carrie and Lowell on March 31 , 2015 in America and received a high score of 9 . 3 from Pitchfork .\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里修改文本以获得输出"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 44,
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in test_sents:\n",
    "    ids = [sent_vocab.stoi[word] for word in sent.split(\" \")]\n",
    "    input_tensor = torch.tensor([ids])\n",
    "    mask = input_tensor != sent_vocab.stoi[\"<pad>\"]\n",
    "    with torch.no_grad():\n",
    "        pred = model.predict(input_tensor, mask)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 45,
>>>>>>> 5a964848d1998a796971fb0f644df79004bf9b30
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sufjan Stevens released his thirteenth album Carrie and Lowell on March 31 , 2015 in America and received a high score of 9 . 3 from Pitchfork . \n",
      " ['B-PER', 'E-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-PER', 'O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-ORG', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(sent, \"\\n\", [tag_vocab.itos[tag_id] for tag_id in pred[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简要地测试了几条文本，可以看出大部分`PER`、`LOC`和`ORG`都可以成功识别，但是对于日期等`MISC`识别效果有待提升"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
