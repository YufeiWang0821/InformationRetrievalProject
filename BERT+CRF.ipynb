{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dataloader import dataset2dataloader\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torchcrf import CRF\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.nn import init\n",
    "from torchtext.legacy import data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_CRF(nn.Module):\n",
    "    def __init__(self, num_tags, device=\"cpu\"):\n",
    "        super(BERT_CRF, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_tags = num_tags\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.hidden2tag = nn.Linear(self.bert.config.hidden_size, num_tags)\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True).to(device)\n",
    "\n",
    "    def forward(self, x, y, mask):\n",
    "        embeddings = self.bert(x)[0]\n",
    "        emissions = self.hidden2tag(embeddings)\n",
    "        loss = -self.crf(emissions=emissions, tags=y, mask=mask)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x, mask=None):\n",
    "        embeddings = self.bert(x)[0]\n",
    "        emissions = self.hidden2tag(embeddings)\n",
    "        preds = self.crf.decode(emissions, mask)\n",
    "        return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_path, debug=False):\n",
    "    train_file_path = os.path.join(dataset_path, \"train.txt\")\n",
    "    dev_file_path = os.path.join(dataset_path, \"dev.txt\")\n",
    "    test_file_path = os.path.join(dataset_path, \"test.txt\")\n",
    "\n",
    "    def process_file(file_path, target_file_path):\n",
    "        sents, tags = [], []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            sent, tag = [], []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if len(line) == 0:\n",
    "                    sents.append(\" \".join(sent))\n",
    "                    tags.append(\" \".join(tag))\n",
    "                    sent, tag = [], []\n",
    "                else:\n",
    "                    splited = line.split(\" \")\n",
    "                    sent.append(splited[0])\n",
    "                    tag.append(splited[-1])\n",
    "            if len(sent) != 0:\n",
    "                sents.append(\" \".join(sent))\n",
    "                tags.append(\" \".join(tag))\n",
    "        df = pd.DataFrame()\n",
    "        df[\"sent\"] = sents if not debug else sents[:100]\n",
    "        df[\"tag\"] = tags if not debug else tags[:100]\n",
    "        df.to_csv(target_file_path, index=False)\n",
    "\n",
    "    train_csv = os.path.join(dataset_path, \"train.csv\") if not debug else os.path.join(dataset_path, \"train_small.csv\")\n",
    "    dev_csv = os.path.join(dataset_path, \"dev.csv\") if not debug else os.path.join(dataset_path, \"train_dev.csv\")\n",
    "    test_csv = os.path.join(dataset_path, \"test.csv\") if not debug else os.path.join(dataset_path, \"train_test.csv\")\n",
    "\n",
    "    if not os.path.exists(test_csv):\n",
    "        process_file(train_file_path, train_csv)\n",
    "        process_file(dev_file_path, dev_csv)\n",
    "        process_file(test_file_path, test_csv)\n",
    "\n",
    "    return train_csv, dev_csv, test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset2dataloader(dataset_path=\"/data/wyf/InformationRetrievalProject/data/\", batch_size=3, debug=False):\n",
    "    train_csv, dev_csv, test_csv = prepare_data(dataset_path, debug=debug)\n",
    "\n",
    "    def tokenizer(text):\n",
    "        return text.split(\" \")\n",
    "\n",
    "    # 这里只是定义了数据格式\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=False)\n",
    "    TAG = data.Field(sequential=True, tokenize=tokenizer, lower=False)\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "        path='', train=train_csv, validation=dev_csv, test=test_csv, format='csv', skip_header=True,\n",
    "        fields=[('sent', TEXT), ('tag', TAG)])\n",
    "\n",
    "    TEXT.build_vocab(train, vectors='glove.6B.50d')  # , max_size=30000)\n",
    "    TAG.build_vocab(val)\n",
    "    \n",
    "    #TAG.build_vocab(test)\n",
    "\n",
    "    # 当 corpus 中有的 token 在 vectors 中不存在时 的初始化方式.\n",
    "    TEXT.vocab.vectors.unk_init = init.xavier_uniform\n",
    "\n",
    "    DEVICE = \"cpu\"\n",
    "    train_iter = data.BucketIterator(train, batch_size=batch_size, sort_key=lambda x: len(x.sent), device=DEVICE)\n",
    "    val_iter = data.BucketIterator(val, batch_size=batch_size, sort_key=lambda x: len(x.sent), device=DEVICE)\n",
    "    test_iter = data.BucketIterator(test, batch_size=batch_size, sort_key=lambda x: len(x.sent), device=DEVICE)\n",
    "\n",
    "\n",
    "    # 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序\n",
    "    # test_iter = data.Iterator(dataset=test, batch_size=128, train=False, sort=False, device=DEVICE)\n",
    "\n",
    "    return train_iter, val_iter, test_iter, TEXT.vocab, TAG.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter, sent_vocab, tag_vocab = dataset2dataloader(batch_size=128)\n",
    "word_vectors = sent_vocab.vectors\n",
    "\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BERT_CRF(num_tags=len(tag_vocab.stoi), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "learning_rate = 0.01\n",
    "model_path = \"model_BertC.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_path):\n",
    "    model = torch.load(model_path)\n",
    "else:\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            x, y = batch.sent.t(), batch.tag.t()\n",
    "            mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(x, y, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                print(f\"epoch:{ep}, iter:{i}, loss:{loss.item()}\", end=\" \")\n",
    "\n",
    "        model.eval()\n",
    "        train_accs = []\n",
    "        preds, golds = [], []\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            x, y = batch.sent.t(), batch.tag.t()\n",
    "            mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "            with torch.no_grad():\n",
    "                preds = model.predict(x, mask)\n",
    "            right, total = 0, 0\n",
    "            for pred, gold in zip(preds, y):\n",
    "                right += np.sum(np.array(pred) == gold[:len(pred)].numpy())\n",
    "                total += len(pred)\n",
    "            train_accs.append(right*1.0/total)\n",
    "        train_acc = np.array(train_accs).mean()\n",
    "\n",
    "        val_accs = []\n",
    "        for i, batch in enumerate(val_iter):\n",
    "            x, y = batch.sent.t(), batch.tag.t()\n",
    "            mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "            with torch.no_grad():\n",
    "                preds = model.predict(x, mask)\n",
    "            right, total = 0, 0\n",
    "            for pred, gold in zip(preds, y):\n",
    "                right += np.sum(np.array(pred) == gold[:len(pred)].numpy())\n",
    "                total += len(pred)\n",
    "            val_accs.append(right * 1.0 / total)\n",
    "        val_acc = np.array(val_accs).mean()\n",
    "        print(\"epoch %d train acc:%.2f, val acc:%.2f\" % (ep, train_acc, val_acc))\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc:0.83\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_accs = []\n",
    "for i, batch in enumerate(test_iter):\n",
    "    x, y = batch.sent.t(), batch.tag.t()\n",
    "    mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "    with torch.no_grad():\n",
    "        preds = model.predict(x, mask)\n",
    "    right, total = 0, 0\n",
    "    for pred, gold in zip(preds, y):\n",
    "        right += np.sum(np.array(pred) == gold[:len(pred)].numpy())\n",
    "        total += len(pred)\n",
    "    test_accs.append(right * 1.0 / total)\n",
    "test_acc = np.array(test_accs).mean()\n",
    "print(\"test acc:%.2f\" % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sents = [\"My name is Yufei Wang , I am from Jinzhou , Hubei , China .\"]\n",
    "#test_sents = [\"HUST is the abbreviation of Huazhong University of Science and Technology . It is located in Hongshan , Wuhan , China .\"]\n",
    "test_sents = [\"Sufjan Stevens released his thirteenth album Carrie and Lowell on March 31 , 2015 in America and received a high score of 9 . 3 from Pitchfork .\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里修改文本以获得输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in test_sents:\n",
    "    ids = [sent_vocab.stoi[word] for word in sent.split(\" \")]\n",
    "    input_tensor = torch.tensor([ids])\n",
    "    mask = input_tensor != sent_vocab.stoi[\"<pad>\"]\n",
    "    with torch.no_grad():\n",
    "        pred = model.predict(input_tensor, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sufjan Stevens released his thirteenth album Carrie and Lowell on March 31 , 2015 in America and received a high score of 9 . 3 from Pitchfork . \n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(sent, \"\\n\", [tag_vocab.itos[tag_id] for tag_id in pred[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简要地测试了几条文本，可以看出大部分`PER`、`LOC`和`ORG`都可以成功识别，但是对于日期等`MISC`识别效果有待提升"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
