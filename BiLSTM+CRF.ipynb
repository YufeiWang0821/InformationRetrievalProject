{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dataloader import dataset2dataloader\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.nn import init\n",
    "from torchtext.legacy import data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_tags, word_vectors=None, device=\"cpu\"):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, _weight=word_vectors).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, bidirectional=True, batch_first=True).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_size*2, num_tags)\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True).to(device)\n",
    "\n",
    "    def get_emissions(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        embedded = self.embed(x)\n",
    "        h0, c0 = torch.zeros(2, batch_size, self.hidden_size).to(self.device), torch.zeros(2, batch_size, self.hidden_size).to(self.device)\n",
    "        lstm_out, (_, _) = self.lstm(embedded, (h0, c0))\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return emissions\n",
    "    \n",
    "    def forward(self, x, y, mask):\n",
    "        emissions = self.get_emissions(x)\n",
    "        loss = -self.crf(emissions=emissions, tags=y, mask=mask)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x, mask=None):\n",
    "        emissions = self.get_emissions(x)\n",
    "        preds = self.crf.decode(emissions, mask)\n",
    "        return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_path, debug=False):\n",
    "    train_file_path = os.path.join(dataset_path, \"train.txt\")\n",
    "    dev_file_path = os.path.join(dataset_path, \"dev.txt\")\n",
    "    test_file_path = os.path.join(dataset_path, \"test.txt\")\n",
    "\n",
    "    def process_file(file_path, target_file_path):\n",
    "        sents, tags = [], []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            sent, tag = [], []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if len(line) == 0:\n",
    "                    sents.append(\" \".join(sent))\n",
    "                    tags.append(\" \".join(tag))\n",
    "                    sent, tag = [], []\n",
    "                else:\n",
    "                    splited = line.split(\" \")\n",
    "                    sent.append(splited[0])\n",
    "                    tag.append(splited[-1])\n",
    "            if len(sent) != 0:\n",
    "                sents.append(\" \".join(sent))\n",
    "                tags.append(\" \".join(tag))\n",
    "        df = pd.DataFrame()\n",
    "        df[\"sent\"] = sents if not debug else sents[:100]\n",
    "        df[\"tag\"] = tags if not debug else tags[:100]\n",
    "        df.to_csv(target_file_path, index=False)\n",
    "\n",
    "    train_csv = os.path.join(dataset_path, \"train.csv\") if not debug else os.path.join(dataset_path, \"train_small.csv\")\n",
    "    dev_csv = os.path.join(dataset_path, \"dev.csv\") if not debug else os.path.join(dataset_path, \"train_dev.csv\")\n",
    "    test_csv = os.path.join(dataset_path, \"test.csv\") if not debug else os.path.join(dataset_path, \"train_test.csv\")\n",
    "\n",
    "    if not os.path.exists(test_csv):\n",
    "        process_file(train_file_path, train_csv)\n",
    "        process_file(dev_file_path, dev_csv)\n",
    "        process_file(test_file_path, test_csv)\n",
    "\n",
    "    return train_csv, dev_csv, test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset2dataloader(dataset_path=\"/data/wyf/InformationRetrievalProject/data/\", batch_size=3, debug=False):\n",
    "    train_csv, dev_csv, test_csv = prepare_data(dataset_path, debug=debug)\n",
    "\n",
    "    def tokenizer(text):\n",
    "        return text.split(\" \")\n",
    "\n",
    "    # 这里只是定义了数据格式\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=False)\n",
    "    TAG = data.Field(sequential=True, tokenize=tokenizer, lower=False)\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "        path='', train=train_csv, validation=dev_csv, test=test_csv, format='csv', skip_header=True,\n",
    "        fields=[('sent', TEXT), ('tag', TAG)])\n",
    "\n",
    "    TEXT.build_vocab(train, vectors='glove.6B.50d')  # , max_size=30000)\n",
    "    TAG.build_vocab(val)\n",
    "    # 下面不注释acc0.89 反之0.95\n",
    "    #TAG.build_vocab(test)\n",
    "\n",
    "    # 当 corpus 中有的 token 在 vectors 中不存在时 的初始化方式.\n",
    "    TEXT.vocab.vectors.unk_init = init.xavier_uniform\n",
    "\n",
    "    DEVICE = \"cpu\"\n",
    "    train_iter = data.BucketIterator(train, batch_size=batch_size, sort_key=lambda x: len(x.sent), device=DEVICE)\n",
    "    val_iter = data.BucketIterator(val, batch_size=batch_size, sort_key=lambda x: len(x.sent), device=DEVICE)\n",
    "    #test_iter = data.BucketIterator(test, batch_size=batch_size, sort_key=lambda x: len(x.sent), device=DEVICE)\n",
    "\n",
    "\n",
    "    # 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序\n",
    "    test_iter = data.Iterator(dataset=test, batch_size=128, train=False, sort=False, device=DEVICE)\n",
    "\n",
    "    return train_iter, val_iter, test_iter, TEXT.vocab, TAG.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter, sent_vocab, tag_vocab = dataset2dataloader(batch_size=128)\n",
    "word_vectors = sent_vocab.vectors\n",
    "\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里由于BiLSTM的数据处理问题，出现了如下报错  \n",
    "`RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor`  \n",
    "这似乎是由于在进行文本嵌入的时候，调用的`nn.Embedding`函数的问题  \n",
    "由于**时间**关系我暂时只能先使`device = torch.device('cpu')`  \n",
    "按照老师所说，后面的NLP专选课实验也会使用BiLSTM+CRF实现NER，届时可能会解决这一问题"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(vocab_size=len(sent_vocab.stoi), embedding_dim=50, hidden_size=128, num_tags=len(tag_vocab.stoi), word_vectors=word_vectors, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "learning_rate = 0.01\n",
    "model_path = \"model_BC.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_path):\n",
    "    model = torch.load(model_path)\n",
    "else:\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            x, y = batch.sent.t(), batch.tag.t()\n",
    "            mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(x, y, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                print(f\"epoch:{ep}, iter:{i}, loss:{loss.item()}\", end=\" \")\n",
    "\n",
    "        model.eval()\n",
    "        train_accs = []\n",
    "        preds, golds = [], []\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            x, y = batch.sent.t(), batch.tag.t()\n",
    "            mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "            with torch.no_grad():\n",
    "                preds = model.predict(x, mask)\n",
    "            right, total = 0, 0\n",
    "            for pred, gold in zip(preds, y):\n",
    "                right += np.sum(np.array(pred) == gold[:len(pred)].numpy())\n",
    "                total += len(pred)\n",
    "            train_accs.append(right*1.0/total)\n",
    "        train_acc = np.array(train_accs).mean()\n",
    "\n",
    "        val_accs = []\n",
    "        for i, batch in enumerate(val_iter):\n",
    "            x, y = batch.sent.t(), batch.tag.t()\n",
    "            mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "            with torch.no_grad():\n",
    "                preds = model.predict(x, mask)\n",
    "            right, total = 0, 0\n",
    "            for pred, gold in zip(preds, y):\n",
    "                right += np.sum(np.array(pred) == gold[:len(pred)].numpy())\n",
    "                total += len(pred)\n",
    "            val_accs.append(right * 1.0 / total)\n",
    "        val_acc = np.array(val_accs).mean()\n",
    "        print(\"epoch %d train acc:%.2f, val acc:%.2f\" % (ep, train_acc, val_acc))\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc:0.94\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_accs = []\n",
    "for i, batch in enumerate(test_iter):\n",
    "    x, y = batch.sent.t(), batch.tag.t()\n",
    "    mask = (x != sent_vocab.stoi[\"<pad>\"])\n",
    "    with torch.no_grad():\n",
    "        preds = model.predict(x, mask)\n",
    "    right, total = 0, 0\n",
    "    for pred, gold in zip(preds, y):\n",
    "        right += np.sum(np.array(pred) == gold[:len(pred)].numpy())\n",
    "        total += len(pred)\n",
    "    test_accs.append(right * 1.0 / total)\n",
    "test_acc = np.array(test_accs).mean()\n",
    "print(\"test acc:%.2f\" % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sents = [\"My name is Yufei Wang , I am from Jinzhou , Hubei , China .\"]\n",
    "#test_sents = [\"HUST is the abbreviation of Huazhong University of Science and Technology . It is located in Hongshan , Wuhan , China .\"]\n",
    "test_sents = [\"Sufjan Stevens released his thirteenth album Carrie and Lowell on March 31 , 2015 in America and received a high score of 9 . 3 from Pitchfork .\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里修改文本以获得输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in test_sents:\n",
    "    ids = [sent_vocab.stoi[word] for word in sent.split(\" \")]\n",
    "    input_tensor = torch.tensor([ids])\n",
    "    mask = input_tensor != sent_vocab.stoi[\"<pad>\"]\n",
    "    with torch.no_grad():\n",
    "        pred = model.predict(input_tensor, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sufjan Stevens released his thirteenth album Carrie and Lowell on March 31 , 2015 in America and received a high score of 9 . 3 from Pitchfork . \n",
      " ['B-PER', 'E-PER', 'O', 'O', 'O', 'B-PER', 'E-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'S-PER', 'O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-ORG', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(sent, \"\\n\", [tag_vocab.itos[tag_id] for tag_id in pred[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简要地测试了几条文本，可以看出大部分`PER`、`LOC`和`ORG`都可以成功识别，但是对于日期等`MISC`识别效果有待提升"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
