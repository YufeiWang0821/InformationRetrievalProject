{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import SequenceTaggingDataset\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy.data import Field\n",
    "from torch.utils.data import DataLoader\n",
    "#from TorchCRF import CRF\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型类\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tag_vocab_size)\n",
    "        self.crf = CRF(tag_vocab_size)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embeds = self.embedding(text)\n",
    "        lstm_out, _ = self.bilstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        return tag_space\n",
    "    \n",
    "    # def loss(self, text, tags):\n",
    "    #     emissions = self.forward(text)\n",
    "    #     loss = self.crf(emissions, tags)\n",
    "    #     return torch.mean(loss)\n",
    "    def loss(self, text, tags):\n",
    "        emissions = self.forward(text)\n",
    "        mask = text.ne(0)\n",
    "        loss =  -self.crf(emissions, tags, mask=mask)\n",
    "        return torch.mean(loss)\n",
    "    \n",
    "    def decode(self, text):\n",
    "        emissions = self.forward(text)\n",
    "        return self.crf.decode(emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "TEXT = Field(lower=True, include_lengths=True, batch_first=True)\n",
    "TAGS = Field(unk_token=None, batch_first=True)\n",
    "train_data, val_data, test_data = SequenceTaggingDataset.splits(\n",
    "    path='/data/wyf/InformationRetrievalProject/data/', train='eng_train.txt', validation='eng_testa.txt', test='eng_testb.txt',\n",
    "    fields=(('text', TEXT), ('tags', TAGS)), separator=' ')\n",
    "TEXT.build_vocab(train_data)\n",
    "TAGS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型参数\n",
    "vocab_size = len(TEXT.vocab)\n",
    "tag_vocab_size = len(TAGS.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "model = BiLSTM_CRF(vocab_size, tag_vocab_size, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代器\n",
    "train_iterator = data.BucketIterator(train_data,batch_size=10,train=True,shuffle=True)\n",
    "val_iterator = data.BucketIterator(val_data,batch_size=len(val_data),train=False,sort=False)\n",
    "test_iterator = data.BucketIterator(test_data,batch_size=len(test_data),train=False,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (embedding): Embedding(21012, 100)\n",
       "  (bilstm): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=256, out_features=47, bias=True)\n",
       "  (crf): CRF(num_tags=47)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "best_accuracy = 0.0\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_6.672%:  Training average Loss: 395.550216\n",
      "Epoch 0_13.345%:  Training average Loss: 303.157243\n",
      "Epoch 0_20.017%:  Training average Loss: 257.670748\n",
      "Epoch 0_26.690%:  Training average Loss: 230.556371\n",
      "Epoch 0_33.362%:  Training average Loss: 209.575772\n",
      "Epoch 0_40.035%:  Training average Loss: 193.945254\n",
      "Epoch 0_46.707%:  Training average Loss: 181.733447\n",
      "Epoch 0_53.380%:  Training average Loss: 171.855697\n",
      "Epoch 0_60.052%:  Training average Loss: 163.659879\n",
      "Epoch 0_66.724%:  Training average Loss: 156.957508\n",
      "Epoch 0_73.397%:  Training average Loss: 150.740822\n",
      "Epoch 0_80.069%:  Training average Loss: 145.227902\n",
      "Epoch 0_86.742%:  Training average Loss: 140.876290\n",
      "Epoch 0_93.414%:  Training average Loss: 136.400189\n",
      "Train done. Now evaluate.\n",
      "Epoch 0 | Val Loss: 8.390 | Val Accuracy: 0.001\n",
      "Epoch 1_6.672%:  Training average Loss: 67.186603\n",
      "Epoch 1_13.345%:  Training average Loss: 65.737945\n",
      "Epoch 1_20.017%:  Training average Loss: 64.547185\n",
      "Epoch 1_26.690%:  Training average Loss: 65.506360\n",
      "Epoch 1_33.362%:  Training average Loss: 63.966324\n",
      "Epoch 1_40.035%:  Training average Loss: 63.442172\n",
      "Epoch 1_46.707%:  Training average Loss: 62.430685\n",
      "Epoch 1_53.380%:  Training average Loss: 61.527831\n",
      "Epoch 1_60.052%:  Training average Loss: 61.230997\n",
      "Epoch 1_66.724%:  Training average Loss: 60.722490\n",
      "Epoch 1_73.397%:  Training average Loss: 60.026711\n",
      "Epoch 1_80.069%:  Training average Loss: 59.191487\n",
      "Epoch 1_86.742%:  Training average Loss: 58.856718\n",
      "Epoch 1_93.414%:  Training average Loss: 58.361787\n",
      "Train done. Now evaluate.\n",
      "Epoch 1 | Val Loss: 6.225 | Val Accuracy: 0.001\n",
      "Epoch 2_6.672%:  Training average Loss: 36.031769\n",
      "Epoch 2_13.345%:  Training average Loss: 38.056740\n",
      "Epoch 2_20.017%:  Training average Loss: 37.629316\n",
      "Epoch 2_26.690%:  Training average Loss: 37.308004\n",
      "Epoch 2_33.362%:  Training average Loss: 37.689383\n",
      "Epoch 2_40.035%:  Training average Loss: 37.531486\n",
      "Epoch 2_46.707%:  Training average Loss: 37.558201\n",
      "Epoch 2_53.380%:  Training average Loss: 37.802294\n",
      "Epoch 2_60.052%:  Training average Loss: 37.551959\n",
      "Epoch 2_66.724%:  Training average Loss: 37.222990\n",
      "Epoch 2_73.397%:  Training average Loss: 37.015189\n",
      "Epoch 2_80.069%:  Training average Loss: 36.995364\n",
      "Epoch 2_86.742%:  Training average Loss: 37.079206\n",
      "Epoch 2_93.414%:  Training average Loss: 36.940469\n",
      "Train done. Now evaluate.\n",
      "Epoch 2 | Val Loss: 5.457 | Val Accuracy: 0.001\n",
      "Epoch 3_6.672%:  Training average Loss: 22.692704\n",
      "Epoch 3_13.345%:  Training average Loss: 23.615714\n",
      "Epoch 3_20.017%:  Training average Loss: 23.813609\n",
      "Epoch 3_26.690%:  Training average Loss: 23.763737\n",
      "Epoch 3_33.362%:  Training average Loss: 23.570677\n",
      "Epoch 3_40.035%:  Training average Loss: 23.485403\n",
      "Epoch 3_46.707%:  Training average Loss: 23.657036\n",
      "Epoch 3_53.380%:  Training average Loss: 23.564145\n",
      "Epoch 3_60.052%:  Training average Loss: 23.683121\n",
      "Epoch 3_66.724%:  Training average Loss: 23.659971\n",
      "Epoch 3_73.397%:  Training average Loss: 23.807709\n",
      "Epoch 3_80.069%:  Training average Loss: 23.938660\n",
      "Epoch 3_86.742%:  Training average Loss: 24.036084\n",
      "Epoch 3_93.414%:  Training average Loss: 24.016246\n",
      "Train done. Now evaluate.\n",
      "Epoch 3 | Val Loss: 5.226 | Val Accuracy: 0.001\n",
      "Epoch 4_6.672%:  Training average Loss: 14.955882\n",
      "Epoch 4_13.345%:  Training average Loss: 14.528963\n",
      "Epoch 4_20.017%:  Training average Loss: 14.250256\n",
      "Epoch 4_26.690%:  Training average Loss: 14.332999\n",
      "Epoch 4_33.362%:  Training average Loss: 14.368216\n",
      "Epoch 4_40.035%:  Training average Loss: 14.394219\n",
      "Epoch 4_46.707%:  Training average Loss: 14.380315\n",
      "Epoch 4_53.380%:  Training average Loss: 14.528964\n",
      "Epoch 4_60.052%:  Training average Loss: 14.816528\n",
      "Epoch 4_66.724%:  Training average Loss: 14.860752\n",
      "Epoch 4_73.397%:  Training average Loss: 14.926185\n",
      "Epoch 4_80.069%:  Training average Loss: 14.950492\n",
      "Epoch 4_86.742%:  Training average Loss: 14.947541\n",
      "Epoch 4_93.414%:  Training average Loss: 14.967954\n",
      "Train done. Now evaluate.\n",
      "Epoch 4 | Val Loss: 5.350 | Val Accuracy: 0.001\n",
      "Epoch 5_6.672%:  Training average Loss: 7.791230\n",
      "Epoch 5_13.345%:  Training average Loss: 8.306301\n",
      "Epoch 5_20.017%:  Training average Loss: 8.019774\n",
      "Epoch 5_26.690%:  Training average Loss: 7.943018\n",
      "Epoch 5_33.362%:  Training average Loss: 8.176999\n",
      "Epoch 5_40.035%:  Training average Loss: 8.367802\n",
      "Epoch 5_46.707%:  Training average Loss: 8.404235\n",
      "Epoch 5_53.380%:  Training average Loss: 8.427700\n",
      "Epoch 5_60.052%:  Training average Loss: 8.395078\n",
      "Epoch 5_66.724%:  Training average Loss: 8.430565\n",
      "Epoch 5_73.397%:  Training average Loss: 8.421606\n",
      "Epoch 5_80.069%:  Training average Loss: 8.591256\n",
      "Epoch 5_86.742%:  Training average Loss: 8.677382\n",
      "Epoch 5_93.414%:  Training average Loss: 8.677040\n",
      "Train done. Now evaluate.\n",
      "Epoch 5 | Val Loss: 5.667 | Val Accuracy: 0.001\n",
      "Epoch 6_6.672%:  Training average Loss: 4.175296\n",
      "Epoch 6_13.345%:  Training average Loss: 4.475120\n",
      "Epoch 6_20.017%:  Training average Loss: 4.405529\n",
      "Epoch 6_26.690%:  Training average Loss: 4.343328\n",
      "Epoch 6_33.362%:  Training average Loss: 4.315616\n",
      "Epoch 6_40.035%:  Training average Loss: 4.278833\n",
      "Epoch 6_46.707%:  Training average Loss: 4.379329\n",
      "Epoch 6_53.380%:  Training average Loss: 4.417902\n",
      "Epoch 6_60.052%:  Training average Loss: 4.473685\n",
      "Epoch 6_66.724%:  Training average Loss: 4.460033\n",
      "Epoch 6_73.397%:  Training average Loss: 4.558624\n",
      "Epoch 6_80.069%:  Training average Loss: 4.627460\n",
      "Epoch 6_86.742%:  Training average Loss: 4.729549\n",
      "Epoch 6_93.414%:  Training average Loss: 4.794563\n",
      "Train done. Now evaluate.\n",
      "Epoch 6 | Val Loss: 6.262 | Val Accuracy: 0.001\n",
      "Epoch 7_6.672%:  Training average Loss: 2.656675\n",
      "Epoch 7_13.345%:  Training average Loss: 2.407629\n",
      "Epoch 7_20.017%:  Training average Loss: 2.370127\n",
      "Epoch 7_26.690%:  Training average Loss: 2.393128\n",
      "Epoch 7_33.362%:  Training average Loss: 2.468390\n",
      "Epoch 7_40.035%:  Training average Loss: 2.485531\n",
      "Epoch 7_46.707%:  Training average Loss: 2.541179\n",
      "Epoch 7_53.380%:  Training average Loss: 2.533745\n",
      "Epoch 7_60.052%:  Training average Loss: 2.580597\n",
      "Epoch 7_66.724%:  Training average Loss: 2.619941\n",
      "Epoch 7_73.397%:  Training average Loss: 2.632458\n",
      "Epoch 7_80.069%:  Training average Loss: 2.662679\n",
      "Epoch 7_86.742%:  Training average Loss: 2.698005\n",
      "Epoch 7_93.414%:  Training average Loss: 2.710393\n",
      "Train done. Now evaluate.\n",
      "Epoch 7 | Val Loss: 6.807 | Val Accuracy: 0.001\n",
      "Epoch 8_6.672%:  Training average Loss: 1.467644\n",
      "Epoch 8_13.345%:  Training average Loss: 1.496275\n",
      "Epoch 8_20.017%:  Training average Loss: 1.453690\n",
      "Epoch 8_26.690%:  Training average Loss: 1.420616\n",
      "Epoch 8_33.362%:  Training average Loss: 1.479349\n",
      "Epoch 8_40.035%:  Training average Loss: 1.489382\n",
      "Epoch 8_46.707%:  Training average Loss: 1.463335\n",
      "Epoch 8_53.380%:  Training average Loss: 1.485794\n",
      "Epoch 8_60.052%:  Training average Loss: 1.483835\n",
      "Epoch 8_66.724%:  Training average Loss: 1.511282\n",
      "Epoch 8_73.397%:  Training average Loss: 1.534064\n",
      "Epoch 8_80.069%:  Training average Loss: 1.552233\n",
      "Epoch 8_86.742%:  Training average Loss: 1.603688\n",
      "Epoch 8_93.414%:  Training average Loss: 1.632844\n",
      "Train done. Now evaluate.\n",
      "Epoch 8 | Val Loss: 7.526 | Val Accuracy: 0.001\n",
      "Epoch 9_6.672%:  Training average Loss: 1.055290\n",
      "Epoch 9_13.345%:  Training average Loss: 1.103854\n",
      "Epoch 9_20.017%:  Training average Loss: 1.031813\n",
      "Epoch 9_26.690%:  Training average Loss: 1.063901\n",
      "Epoch 9_33.362%:  Training average Loss: 1.062643\n",
      "Epoch 9_40.035%:  Training average Loss: 1.024807\n",
      "Epoch 9_46.707%:  Training average Loss: 1.069324\n",
      "Epoch 9_53.380%:  Training average Loss: 1.145984\n",
      "Epoch 9_60.052%:  Training average Loss: 1.205150\n",
      "Epoch 9_66.724%:  Training average Loss: 1.246608\n",
      "Epoch 9_73.397%:  Training average Loss: 1.283924\n",
      "Epoch 9_80.069%:  Training average Loss: 1.283279\n",
      "Epoch 9_86.742%:  Training average Loss: 1.311516\n",
      "Epoch 9_93.414%:  Training average Loss: 1.350690\n",
      "Train done. Now evaluate.\n",
      "Epoch 9 | Val Loss: 7.451 | Val Accuracy: 0.001\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    total_correct = 0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    \n",
    "    \n",
    "    for batch in train_iterator:\n",
    "        steps += 1\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        tags = batch.tags\n",
    "        text = text.to(device)\n",
    "        tags = tags.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(text, tags)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps%100==0:\n",
    "            print(\"Epoch %d_%.3f%%:  Training average Loss: %f\"\n",
    "                      %(epoch, steps * train_iterator.batch_size*100/len(train_iterator.dataset),total_loss/steps))\n",
    "    \n",
    "    print(\"Train done. Now evaluate.\")\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0.0\n",
    "        total_correct = 0.0\n",
    "        steps = 0.0\n",
    "        for batch in val_iterator:\n",
    "            steps += 1\n",
    "            text, text_lengths = batch.text\n",
    "            tags = batch.tags\n",
    "            text = text.to(device)\n",
    "            tags = tags.to(device)\n",
    "            emissions = model.forward(text)\n",
    "            predicted_tags = model.decode(text)\n",
    "            loss = -model.crf(emissions, tags)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            total_tokens += text_lengths.sum().item()\n",
    "            #total_correct += sum([1 for i in range(len(predicted_tags)) for j in range(len(predicted_tags[i])) if predicted_tags[i][j] == tags[i][j]])\n",
    "            for i in range(len(predicted_tags)):\n",
    "                #for j in range(len(predicted_tags[i])):\n",
    "                for j in range(text_lengths[i]):\n",
    "                    if predicted_tags[i][j] == tags[j][i]:\n",
    "                        total_correct += 1\n",
    "            \n",
    "        val_loss = total_loss / len(val_data)\n",
    "        val_accuracy = total_correct / total_tokens\n",
    "        print(f'Epoch {epoch} | Val Loss: {val_loss:.3f} | Val Accuracy: {val_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450968\n",
      "46666\n",
      "456816\n",
      "Test Loss: 8.170 | Test Accuracy: 0.987\n"
     ]
    }
   ],
   "source": [
    "# 在测试集上评估模型\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    total_size = 0\n",
    "    total_correct = 0\n",
    "    for batch in test_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        tags = batch.tags\n",
    "        text = text.to(device)\n",
    "        tags = tags.to(device)\n",
    "        emissions = model.forward(text)\n",
    "        predicted_tags = model.decode(text)\n",
    "        loss = -model.crf(emissions, tags)\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += text_lengths.sum().item()\n",
    "        \n",
    "        #total_correct += sum([1 for i in range(len(predicted_tags)) for j in range(len(predicted_tags[i])) if predicted_tags[i][j] == tags[i][j]])\n",
    "        for i in range(len(predicted_tags)):\n",
    "            for j in range(len(predicted_tags[i])):\n",
    "                total_size += 1\n",
    "            #for j in range(text_lengths[i]):\n",
    "                if predicted_tags[i][j] == tags[j][i]:\n",
    "                    total_correct += 1\n",
    "    test_loss = total_loss / len(test_data)\n",
    "    #test_accuracy = total_correct / total_tokens\n",
    "    test_accuracy = total_correct / total_size\n",
    "    print(total_correct)\n",
    "    print(total_tokens)\n",
    "    print(total_size)\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Accuracy: {test_accuracy:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
